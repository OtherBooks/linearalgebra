<section>
  <title>Composition and Matrix Multiplication</title>
  <p>
    We previously defined the composition of linear transformation.
    Compositon allows us to combine transformations:
    we can ask what happens if we first rotate and then reflect in <m>\RR^2</m>.
    However, since matrices represent transformations,
    this compsotion should somehow be accounted for in the matrix representation.
    If <m>A</m> is the matrix of <m>S</m> and <m>B</m> is the matrix of <m>T</m>,
    what is the matrix of <m>S \circ T</m>?
    The answer is given by matrix multiplication.
  </p>
  <definition>
    <statement>
      <p>
        Let <m>A</m> be a <m>k \times m</m> matrix and <m>B</m> a <m>m \times n</m> matrix.
        We can think of the <term>rows</term>
        of <m>A</m> as vectors in <m>\RR^m</m>,
        and the <term>columns</term> of <m>B</m> as vectors in <m>\RR^m</m> as well.
        To emphasise this perspective, we write the following,
        using <m>u_i</m> for the rows of <m>A</m> and <m>v_i</m> for the columns of <m>B</m>.
        <me>
          A = \left( \begin{matrix} \rightarrow \amp  u_1 \amp  \rightarrow \\ \rightarrow \amp  u_2 \amp  \rightarrow \\ \rightarrow \amp  u_3 \amp  \rightarrow \\ \vdots \amp  \vdots \amp  \vdots \\ \rightarrow \amp  u_k \amp  \rightarrow \end{matrix} \right) \hspace{1cm} B = \left( \begin{matrix} \downarrow \amp  \downarrow \amp  \downarrow \amp  \ldots \amp  \downarrow \\ v_1 \amp  v_2 \amp  v_3 \amp  \ldots \amp  v_n \\ \downarrow \amp  \downarrow \amp  \downarrow \amp  \ldots \amp  \downarrow \end{matrix} \right)
        </me>
      </p>
      <p>
        With this notation, the <term>matrix multiplication</term>
        of <m>A</m> and <m>B</m> is the
        <m>k \times n</m> matrix where the entires are the dot products of rows and columns.
        <me>
          AB = \left( \begin{matrix} u_1 \cdot v_1 \amp u_1 \cdot v_2 \amp u_1 \cdot v_3 \amp \ldots \amp u_1 \cdot v_n \\ u_2 \cdot v_1 \amp u_2 \cdot v_2 \amp u_2 \cdot v_3 \amp \ldots \amp u_2 \cdot v_n \\ u_3 \cdot v_1 \amp u_3 \cdot v_2 \amp u_3 \cdot v_3 \amp \ldots \amp u_3 \cdot v_n \\ \vdots \amp  \vdots \amp  \vdots \amp  \ldots \amp  \vdots \\ u_k \cdot v_1 \amp u_k \cdot v_2 \amp u_k \cdot v_3 \amp \ldots \amp u_k \cdot v_n \end{matrix} \right)
        </me>
      </p>
    </statement>
  </definition>
  <p>
    This operation has the desired property:
    the product of the matrices repesents the composition of the transformations.
    (This remarkable fact is presented here without proof; I'll leave it to you to wonder why this wierd combinations of dot products has the desired geometric interpretation.)
    Remember that the composition still works from right to left,
    so that the matrix multiplication <m>AB</m> represents the transformation associated to <m>B</m> first,
    followed by the transformation associated to <m>A</m>.
    When we think of matrices acting on vectors,
    we wrote the action on the right: <m>Av</m>.
    Now when when a composition acts,
    as in <m>ABv</m>, the closest matrix gets to act first.
  </p>
  <p>
    We have defined a new algebraic operation.
    As with the new products for vectors
    (dot and cross),
    we want to know the properties of this new operation.
  </p>
  <proposition>
    <statement>
      <p>
        Let <m>A</m> be a <m>k \times l</m> matrix,
        <m>B</m> and <m>C</m> be <m>l \times m</m> matrices,
        and <m>D</m> a <m>m \times n</m> matrix.
        Also let <m>I_i</m> be the identity matrix in <m>\RR^i</m> for any <m>i \in \NN</m>.
        Then there are three important properties of matrix multiplication.
        <men>
          \begin{array}{rcll} A(BD) \amp  = \amp  (AB) D \hspace{1cm} \amp  \text{ Associative }  \\ A(B+C) \amp  = \amp  AB + AC \hspace{1cm} \amp  \text{ Distributive }  \\ AI_l \amp  = \amp  I_kA = A \hspace{1cm} \amp  \text{ Identity } \end{array}
        </men>
      </p>
    </statement>
  </proposition>
  <p>
    Note the lack of commutativity: <m>AB \neq BA</m>.
    In fact, if <m>A</m> and <m>B</m> are not square matrices,
    if <m>AB</m> is defined and <m>BA</m> will not be;
    the indices will not match.
    Not only are we unable to exchange the order of matrix multiplication;
    sometimes that multiplication doesn't even make sense as an operation.
    Matrix multiplication is a very important example of a non-commutative product.
  </p>
</section>